# -*- coding: utf-8 -*-
"""DAP_Final_Assessment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XQB4nGcl9218UeTUiiNl_Lj_yTFVlwjk

# Data Analysis

## Importing Tools

To analyse the data, the analyst will require the use of pre-written algorithms. To ensure that the algorithms and their associated libraries are easy to find, it is standard to co-locate all required algorithms in one box at the beginning of the coding script.
"""

!pip install yellowbrick
!pip install scikit-learn

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import math
import matplotlib.ticker as ticker
import time
import xgboost as xgb


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, make_scorer, silhouette_score
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, silhouette_samples, roc_curve, auc, roc_auc_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer

import warnings
warnings.filterwarnings('ignore')

"""## Loading the Dataset

For this project, an open source dataset from kaggle will be used. The dataset can be obtained from the following location:

https://www.kaggle.com/datasets/mysarahmadbhat/airline-passenger-satisfaction/data

The dataset is a collection of customer satisfaction surveys for people who have travelled as a passenger on a commercial airline.

As the coding medium used is 'Google Colab', the dataset has been storedd on Github and will be pulled from that location for the analysis.
"""

file_path = "https://raw.githubusercontent.com/OCE1984/MScDataAnaltyicsPrinciples/main/airline_passenger_satisfaction.csv"
df_original = pd.read_csv(file_path)

"""## Exploratory Data Analysis (EDA)

Once the dataset is loaded into the coding environment, the dataset must be analysed to determine the size, shape and content of the dataset. This step is known as Exploratory Data Analysis (EDA) and helps the analyst to understand the dataset before any analysis is carried out.

This will not only direct the analyst in the most suitable types of analysis to use, but also determine whether or not the dataset is configured correctly for data analysis.
"""

print("\nDataset Info:")
df_original.info()

"""By looking at the 'info' of the dataset, it's possible to see the structure and the content of the dataset.

First, the number of records contained within the dataset is 129,880 entries. It is recommended that a dataset has a minimum of 500 entries for machine learning algorithms to be effective, so the dataset can be used for machine learning purposes. This is to ensure that there is enough data to split into training and test subsets of the data and still provide enough different combinations of possible data configurations to identify trends.

Next, it's possible to see that there are 24 columns in total, as well as the name of each of the columns within the dataset. This is extremely useful as it allows the analyst to transform the data within any given column contained in the dataset.

Then, it can also be seen how many 'non-null' values there are in each column. If the value is less than the number of entries in the dataset, then that column is gong to have missing values. This can be verified by running a 'missing values' code to clearly show where any missing entries are for any given column.
"""

print("\nMissing Values:")
print(df_original.isnull().sum())

"""The 'missing values' code has confirmed that there are 393 missing values from the 'Arrival Delay' column. Before any analysis can be carried out, the null-values will have to be addressed to ensure that they don't influence the accuracy of any modelling carried out. This will be addressed in the next section 'Data Cleaning'.

Finally, the data type (DType), is visible to show what kind of data each column has. The 'Arrival Delay' is a float format, so this will have to be converted to an integer format in the 'Data Cleaning' section.

Now that the dataset is better understood, it is good practice to create a 'Data Dictionary'. The purpose of the dictionary to is communicate to anyone else who needs to interact with the dataset. It explains all the columns and provides the context behind the variables within the columns.
"""

# Generate a Data Dictionary to explain the dataset to others

column_descriptions = {
    "ID": "Unique identifier for each passenger",
    "Gender": "Passenger's gender (Male/Female)",
    "Age": "Passenger's age in years",
    "Customer Type": "Type of customer (Loyal or Disloyal customer)",
    "Type of Travel": "Purpose of flight (Personal or Business)",
    "Class": "Ticket class (Economy, Premium Economy, Business)",
    "Flight Distance": "Distance of the flight in miles",
    "Departure Delay": "Departure delay duration (minutes)",
    "Arrival Delay": "Arrival delay duration (minutes)",
    "Departure and Arrival Time Convenience": "Rating for convenience of departure/arrival times",
    "Ease of Online Booking": "Rating of the ease of online booking, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Check-in Service": "Rating of check-in service quality, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Online Boarding": "Rating of online boarding experience, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Gate Location": "Rating for convenience/location of gate, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "On-board Service": "Rating for onboard customer service, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Seat Comfort": "Rating for comfort of the seats, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Leg Room Service": "Rating for legroom availability, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Cleanliness": "Rating for cleanliness of the aircraft, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Food and Drink": "Rating for onboard food and drink quality, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "In-flight Service": "Rating for overall inflight service, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "In-flight Wifi Service": "Rating for onboard wifi service quality, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "In-flight Entertainment": "Rating for inflight entertainment options, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Baggage Handling": "Rating for baggage handling experience, from 1 (lowest) to 5 (highest) - 0 means 'not applicable'",
    "Satisfaction": "Overall passenger satisfaction (Satisfied/Neutral or Dissatisfied)",
}

data_dict = pd.DataFrame({
    "Column Name": df_original.columns,
    "Data Type": df_original.dtypes.values,
    "Description": [column_descriptions.get(col, "No description provided") for col in df_original.columns]
})

data_dict.to_csv("completed_data_dictionary.csv", index=False)

print("\nSummary Statistics:")
df_original.describe()

print("\nSummary Statistics:")
summary_stats = df_original.describe().T
summary_stats = summary_stats.round(1)
print(summary_stats)

# Save to CSV
summary_stats.to_csv("summary_stats.csv", index=True)
print("Summary statistics saved to summary_stats.csv")

# Create list of unique values within the categorical type columns
categorical_cols = df_original.select_dtypes(include=['object']).columns

for col in categorical_cols:
    unique_values = df_original[col].unique()
    print(f"Unique values for '{col}': {unique_values}")

# Create a table to show the spread of the data across those unique values
summary_table = pd.DataFrame()

for col in categorical_cols:
    counts = df_original[col].value_counts().reset_index()
    counts.columns = ['Level', 'Count']
    counts['Percentage'] = (counts['Count'] / counts['Count'].sum()) * 100
    counts['Variable'] = col
    summary_table = pd.concat([summary_table, counts], ignore_index=True)


summary_table = summary_table[['Variable', 'Level', 'Count', 'Percentage']]
summary_table

# Visualise the unique values in graphs
categorical_cols = df_original.select_dtypes(include='object').columns

# Create the grid
num_cols = 2
num_rows = math.ceil(len(categorical_cols) / num_cols)
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4))
axes = axes.flatten()
max_count = 0

# Iterate and plot on subplots
for i, col in enumerate(categorical_cols):
    sns.countplot(data=df_original, x=col, palette="pastel", ax=axes[i])
    axes[i].set_title(f"Distribution of {col}")
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Count")
    axes[i].tick_params(axis='x', rotation=45)

    # Update max_count if current plot has a higher count
    current_max = axes[i].get_ylim()[1]
    if current_max > max_count:
        max_count = current_max

# Set ylim for all subplots to the maximum count
for ax in axes:
    ax.set_ylim(0, max_count)

# Hide any unused subplots
for i in range(len(categorical_cols), len(axes)):
    axes[i].set_visible(False)

plt.tight_layout()
plt.show()

# Create histograms of the Departure_Delay and Arival_Delay
numerical_cols = df_original.select_dtypes(include=['int64', 'float64']).columns

for col in ['Departure Delay','Arrival Delay']:
    plt.figure(figsize=(8, 6))
    sns.histplot(data=df_original, x=col)
    plt.title(f'Histogram of {col}')
    plt.show()

# Generate histograms of Age and Flight Distance
numerical_cols = ['Age', 'Flight Distance']

# Establish subplot grid
num_cols = 2
num_rows = math.ceil(len(numerical_cols) / num_cols)
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4))
axes = axes.flatten()

# Plot histograms with KDE overlayed
for i, col in enumerate(numerical_cols):
    sns.histplot(data=df_original, x=col, ax=axes[i],
                 color='skyblue', alpha=0.5, kde=False, stat='density')
    sns.kdeplot(data=df_original, x=col, ax=axes[i], color='darkblue', linewidth=2)
    axes[i].set_title(f"Distribution of {col}")
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Density")


for j in range(len(numerical_cols), len(axes)):
    axes[j].set_visible(False)


plt.tight_layout()
plt.show()

# Create histplots for the ratings columns
columns_to_plot = [
    'Departure and Arrival Time Convenience',
    'Ease of Online Booking',
    'Check-in Service',
    'Online Boarding',
    'Gate Location',
    'On-board Service',
    'Seat Comfort',
    'Leg Room Service',
    'Cleanliness',
    'Food and Drink',
    'In-flight Service',
    'In-flight Wifi Service',
    'In-flight Entertainment',
    'Baggage Handling'
]

# Create the subplot grid
n_cols = 4
n_rows = math.ceil(len(columns_to_plot) / n_cols)
fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
axes = axes.flatten()

# Create the histplots for each column
for i, col in enumerate(columns_to_plot):
  sns.histplot(df_original[col], ax=axes[i])
  axes[i].set_title(col)
  axes[i].set_xlabel("")

# Hide any unused subplots
for j in range(len(columns_to_plot), len(axes)):
  axes[j].set_visible(False)

plt.tight_layout()
plt.show()

"""We can see 14 integer variables are satisfaction scores varying from 1-5 (with 0 = non applicable):

Departure and Arrival Time Convenience
Ease of Online Booking
Check-in Service
Online Boarding
Gate Location
On Board Service
Seat Comfort
Leg Room Service
Cleanliness
Food and Drink
In-Flight service
In-Flight WiFi service
In-Flight Entertainment
Baggage Handling
"""

# Generate the Heatmap for the continuous features
numerical_cols = ['Age', 'Flight Distance', 'Departure Delay', 'Arrival Delay']

plt.figure(figsize=(12, 6))
sns.heatmap(df_original[numerical_cols].corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Continuous Feature Correlation Heatmap")
plt.show()

"""## Cleaning the Data"""

# Remove the ID column
df_clean = df_original.copy()
df_clean.drop(columns=["ID"], inplace=True)

# Replace missing Arrival Delay with Departure Delay values
df_clean["Arrival Delay"].fillna(df_clean["Departure Delay"], inplace=True)
df_clean["Arrival Delay"] = df_clean["Arrival Delay"].round().astype(int)

# Verify that there are no more missing values
print("Remaining missing Arrival Delay values:", df_clean["Arrival Delay"].isnull().sum())
print("Data type of Arrival Delay:", df_clean["Arrival Delay"].dtype)

"""####Scaling"""

# Scaling the numerical columns for use in Machine Learning algorithms
numerical_cols = ['Age', 'Flight Distance', 'Departure Delay', 'Arrival Delay']

scaler = StandardScaler()
df_clean[numerical_cols] = scaler.fit_transform(df_clean[numerical_cols])

print(df_clean[numerical_cols].head())

"""#### **Categorical Variables: Binarisation (0/1) and One Hot Encoding**

Columns with categories were identified as pandas type 'Object'.

In order to make these categorical variables processable by the machine learning algorithms, they need to be converted to numbers. I will convert the categories of binary variables directly to 0 and 1, while for variables with more than one category I will use one-hot encoding.

One-hot encoding is a technique used in data preprocessing to convert categorical data (with more than 2 categories) into a numerical format.

Step 1: extract binary categorical variables to convert them to 0 and 1
"""

# Convert binary columns to 1's and 0's
encoding_mappings = []

object_cols = df_clean.select_dtypes(include=['object']).columns

# Filter for columns with 2 or fewer unique values
binary_categorical_cols = [col for col in object_cols
                           if df_clean[col].nunique() <= 2]

# Convert binary categorical columns to 0 and 1
for col in binary_categorical_cols:
    unique_values = df_clean[col].unique()
    mapping = {unique_values[0]: 0, unique_values[1]: 1}
    df_clean[col] = df_clean[col].map(mapping)
    encoding_mappings.extend([[col, val, encoded] for val, encoded in mapping.items()])

legend_table = pd.DataFrame(encoding_mappings, columns=["Column Name", "Original Value", "Encoded Value"])

print(legend_table)

# Check column names
for col in df_clean.columns:
  print(col)

"""Step 2: extract categorical variables with more than two categories and One-hot Encode them"""

# Identify columns to One-hot Encode
cols_to_Onehot_Encode = [col for col in df_clean.select_dtypes(include='object') if df_clean[col].nunique() > 2]

cols_to_Onehot_Encode.extend(['Departure and Arrival Time Convenience','Ease of Online Booking',
       'Check-in Service', 'Online Boarding', 'Gate Location',
       'On-board Service', 'Seat Comfort', 'Leg Room Service', 'Cleanliness',
       'Food and Drink', 'In-flight Service', 'In-flight Wifi Service',
       'In-flight Entertainment', 'Baggage Handling'])

cols_to_Onehot_Encode

# Carry out the One-hot Encoding
encoder = OneHotEncoder(sparse_output=False)
one_hot_encoded = encoder.fit_transform(df_clean[cols_to_Onehot_Encode])

encoded_column_names = encoder.get_feature_names_out(cols_to_Onehot_Encode)

one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoded_column_names)

# Collate columns back into data frame
df_clean = pd.concat([df_clean, one_hot_df], axis=1)

# Drop the original categorical columns
df_clean = df_clean.drop(columns=cols_to_Onehot_Encode)
df_clean.columns

# Create a table to identify the changes related to one-hot encoding
changes = []

for original_col in cols_to_Onehot_Encode:
    original_values = df_original[original_col].unique()

    for original_val in original_values:
        new_col_name = original_col + '_' + str(original_val)

        changes.append([original_col, original_val, new_col_name,
                        f"1 if the original value was '{original_val}' else 0"])

change_table = pd.DataFrame(changes, columns=["Original Column",
                                              "Original Value",
                                              "New Column",
                                              "New Value Representation"])

# Save the change table to a CSV file
change_table.to_csv('one_hot_encoding_changes.csv', index=False)
print("One-hot encoding changes saved to one_hot_encoding_changes.csv")

"""## Feature Engineering

## Supervised Machine Learning

- Partition data into training and test set
- Use random forest as classifier X-predictors (all variables) Y-Satisfaction

x1 + x2 + x3 + x4 = Y (satisfied/not satisfied)
- Fit model on training set: X_training set (matrix) Y_training column

- Test the model: feed to the model the X_test set to predict the Y_test column
- Evaluate model (accuracy, precision, f1-score, confusion matrix, AUROC)

#### Random Forest
"""

# Using Random Forest to predict Satisfaction
X = df_clean.drop('Satisfaction', axis=1)
y = df_clean['Satisfaction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV (Random Forest Classifier)
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid={'n_estimators': [100, 150], 'max_depth': [None, 10]},
                           cv=3, scoring='accuracy', n_jobs=-1)

grid_search.fit(X_train, y_train)

# Display best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)

print("Classification Report on Test Data:")
print(classification_report(y_test, y_pred))

# Create Confusion mattrix to assess model performance
display = ConfusionMatrixDisplay.from_estimator(
    best_rf,
    X_test,
    y_test,
    cmap='Blues',
    display_labels=['Not Satisfied', 'Satisfied']
)
display.ax_.set_title("Confusion Matrix")
display.ax_.grid(False)
display.ax_.xaxis.set_ticklabels(['Predicted Not Satisfied', 'Predicted Satisfied'])
display.ax_.yaxis.set_ticklabels(['Actual Not Satisfied', 'Actual Satisfied'])

plt.show()

# Calculate ROC AUC score
y_pred_proba = best_rf.predict_proba(X_test)[:, 1]

roc_auc = roc_auc_score(y_test, y_pred_proba)

print("ROC AUC Score:", roc_auc)

# Visualise the ROC Curve
y_pred_proba = grid_search.best_estimator_.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba, pos_label=1)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
sns.set_theme(style="darkgrid")
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkblue', linewidth=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', linewidth=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC Curve)')
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# Identify the Top 5 predictors the algorithm determined
best_rf = grid_search.best_estimator_

importances = best_rf.feature_importances_
feature_names = X_train.columns

feature_importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(5))
plt.title('Top 5 Feature Importances (Best Random Forest)')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

"""#### Extreme Gradient Boosting"""

# Using Extreme Gradient Boosting to see if the model performs better
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
}

xgb_classifier = xgb.XGBClassifier(random_state=42)
grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid,
                           scoring='accuracy', cv=3, n_jobs=-1)

grid_search.fit(X_train, y_train)
best_xgb_classifier = grid_search.best_estimator_
y_pred = best_xgb_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(classification_report(y_test, y_pred, target_names=['0', '1']))

fig, ax = plt.subplots()

cm_display = ConfusionMatrixDisplay.from_estimator(
    best_xgb_classifier,
    X_test,
    y_test,
    cmap='Blues',
    ax=ax,
    display_labels=['Not Satisfied', 'Satisfied']
)

ax.grid(False)
plt.show()

# Obtaining the ROC score for XGB Boost
y_pred_proba = best_xgb_classifier.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_pred_proba)
print("ROC AUC Score:", roc_auc)

def evaluate_model(model, X_test, y_test):
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)
  roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

  return accuracy, precision, recall, f1, roc_auc

param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid_rf,
                           cv=3, scoring='accuracy', n_jobs=-1)

grid_search_rf.fit(X_train, y_train)

rf_classifier = RandomForestClassifier(random_state=42,
                                       **grid_search_rf.best_params_)
rf_classifier.fit(X_train, y_train)


param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
}

xgb_classifier = xgb.XGBClassifier(random_state=42)
grid_search_xgb = GridSearchCV(estimator=xgb_classifier,
                               param_grid=param_grid_xgb,
                           scoring='accuracy', cv=3, n_jobs=-1)

grid_search_xgb.fit(X_train, y_train)
best_xgb_params = grid_search_xgb.best_params_




rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc = evaluate_model(
    model=rf_classifier, X_test=X_test, y_test=y_test)


xgb_classifier = xgb.XGBClassifier(random_state=42, **best_xgb_params)
xgb_classifier.fit(X_train, y_train)
xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_roc_auc = evaluate_model(
    model=xgb_classifier, X_test=X_test, y_test=y_test)

# Create a table for comparison
performance_metrics = pd.DataFrame({
    'Model': ['Random Forest', 'XGBoost'],
    'Accuracy': [rf_accuracy, xgb_accuracy],
    'Precision': [rf_precision, xgb_precision],
    'Recall': [rf_recall, xgb_recall],
    'F1-Score': [rf_f1, xgb_f1],
    'ROC AUC': [rf_roc_auc, xgb_roc_auc]
})

performance_metrics

# Plotting the two models against each other to compare them
y_pred_proba_rf = best_rf.predict_proba(X_test)[:, 1]
y_pred_proba_xgb = best_xgb_classifier.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC for Random Forest
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_proba_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Calculate ROC curve and AUC for XGBoost
fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_pred_proba_xgb)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

plt.figure(figsize=(8, 6))
sns.lineplot(x=fpr_rf, y=tpr_rf, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
sns.lineplot(x=fpr_xgb, y=tpr_xgb, color='red', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')
sns.lineplot(x=[0, 1], y=[0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.show()

# Calculate Top 5 feature importance for XGB Boost
importances = best_xgb_classifier.feature_importances_
feature_names = X_train.columns

# Create a DataFrame for feature importances
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(5))
plt.title('Top 5 Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Generate a function to iterate through to create the following graphs
def plot_satisfaction_by_category(df, categorical_column):
    plt.figure(figsize=(10, 6))
    ax = sns.countplot(y=categorical_column, hue='Satisfaction',
                      palette={'Satisfied': 'b', 'Neutral or Dissatisfied': 'navy'},
                      data=df)

    neutral_or_dissatisfied_patches = [p for p in ax.patches
                                       if p.get_facecolor() == sns.color_palette("tab10")[0]]

    for patch in neutral_or_dissatisfied_patches:
        patch.set_alpha(0.5)

    counts = df[categorical_column].value_counts().to_dict()
    plt.gca().set_yticklabels([label.get_text() for label in plt.gca().get_yticklabels()])

    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:.0f}K'.format(x/1000)))

    plt.title(f'Satisfaction by {categorical_column}')
    plt.xlabel('Count')
    plt.ylabel(categorical_column)
    ax.tick_params(left=False)
    ax.tick_params(bottom=False)
    plt.legend(title='Satisfaction')
    plt.show()

plot_satisfaction_by_category(df_original, 'Online Boarding')

plot_satisfaction_by_category(df_original, 'Type of Travel')

plot_satisfaction_by_category(df_original, 'In-flight Wifi Service')

"""### Unsupervised Learning

Identify customer segments based on customer demographics and the top 5 predictors of XGBoost model.

Scaling is required for Unsupervised Learning as values in different scales would would affect the results. In this dataset, the scales are,for example:

	•	Age (0–100 years)
	•	Flight Distance (0–10,000 miles)
	•	Satisfaction Ratings (1–5 points)

Without scaling, the flight distance would have a distorted level of importance in the cluster, skewing the results and potentially making them misleading.
"""

df_clean.head()

# Preparing the data for Unsupervised Learning
clustering_features = ['Gender', 'Age', 'Type of Travel',
                       'Online Boarding_5', 'In-flight Wifi Service_5', 'Satisfaction']

X_cluster = df_clean[clustering_features]

# Calculate silhouette score using a sample of data points to reduce computational load
sample_size = 1000
sample_indices = np.random.choice(X_cluster.shape[0], size=sample_size, replace=False)

# Range of cluster values to test
k_values = range(2, 11)
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(X_cluster)
    score = silhouette_score(X_cluster.iloc[sample_indices], cluster_labels[sample_indices])
    silhouette_scores.append(score)

# Plot the graph to show the Silhouette Score
plt.figure(figsize=(8, 6))
sns.lineplot(x=k_values, y=silhouette_scores, marker='o', color='steelblue')

# Highlight the highest silhouette score
max_score_idx = silhouette_scores.index(max(silhouette_scores))
best_k = k_values[max_score_idx]
best_score = silhouette_scores[max_score_idx]

# Annotate the optimal k clearly on the plot
plt.axvline(x=best_k, linestyle='--', color='red', alpha=0.7)
plt.text(best_k, best_score, f'  Best k = {best_k}\n  Score = {best_score:.2f}',
         verticalalignment='bottom', color='red', fontsize=10, fontweight='bold')

plt.title('Silhouette Score vs. Number of Clusters (k)')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.xticks(k_values)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""As the dataset is quite large, a sample of the dataset will be used to generate the images of the silhouette diagrams for 2 to 7 clusters to show how the clusters would be grouped."""

# Creating a diagram to show the spread of the data in each of the number of clusters
X_sample = X_cluster.sample(n=5000, random_state=42)

fig, ax = plt.subplots(3, 2, figsize=(15, 8))

for i in [2, 3, 4, 5, 6, 7]:
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q - 1][mod])
    visualizer.fit(X_sample)
    visualizer.ax.set_title(f"Silhouette Plot for k = {i}")
    visualizer.ax.set_xlabel("Silhouette Coefficient Values")
    visualizer.ax.set_ylabel("Cluster Label")
    avg_score = visualizer.silhouette_score_
    visualizer.ax.text(0.05, 0.95, f"Average Score: {avg_score:.2f}",
                       transform=visualizer.ax.transAxes, color="red", fontsize=10)

plt.tight_layout()
plt.show()

# Generating an Elbow Plot for KMeans Clustering
inertia_values = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_cluster)
    inertia_values.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
sns.lineplot(x=k_values, y=inertia_values, marker='o', color='steelblue')

elbow_k = 4
elbow_inertia = inertia_values[k_values.index(elbow_k)]

plt.axvline(x=elbow_k, linestyle='--', color='red', alpha=0.7)
plt.text(elbow_k, elbow_inertia, f'  Elbow at k={elbow_k}',
         verticalalignment='top', color='red', fontsize=12, fontweight='bold')
plt.title('Elbow Plot for KMeans Clustering')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_values)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Generating a Distortion Score graph
k_values = range(2, 10)
distortion_scores = []
fit_times = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    start_time = time.time()
    kmeans.fit(X_cluster)
    end_time = time.time()

    distortion_scores.append(kmeans.inertia_)
    fit_times.append(end_time - start_time)


fig, ax1 = plt.subplots(figsize=(10, 6))

ax1.plot(k_values, distortion_scores, marker='o', color='teal', linewidth=2)
ax1.set_xlabel('k', fontsize=12)
ax1.set_ylabel('distortion score', color='teal', fontsize=12)
ax1.tick_params(axis='y', labelcolor='teal')

ax2 = ax1.twinx()
ax2.plot(k_values, fit_times, marker='o', color='olive', linestyle='--', linewidth=2)
ax2.set_ylabel('fit time (seconds)', color='olive', fontsize=12)
ax2.tick_params(axis='y', labelcolor='olive')


elbow_k = 4
ax1.axvline(x=elbow_k, linestyle='--', color='black', linewidth=2)
ax1.text(elbow_k + 0.1, distortion_scores[elbow_k - k_values[0]],
         f'$elbow at$ k = {elbow_k}, $score$ = {distortion_scores[elbow_k - k_values[0]]:.2f}',
         fontsize=12, fontstyle='italic', color='black')


plt.title('Distortion Score Elbow for KMeans Clustering', fontsize=14)
fig.tight_layout()
ax1.grid(True, linestyle='--', alpha=0.5)
plt.show()

# Create a KMeans instance with k=2 and a random state for reproducibility
kmeans = KMeans(n_clusters=2, random_state=42)

# Fit the model to your data
kmeans.fit(X_cluster)

# Add the cluster labels for each data point
cluster_labels = kmeans.labels_
X_cluster['Cluster'] = cluster_labels

df_clean['Cluster'] = cluster_labels

# Create a function to iterate through to create the following graphs
def plot_cluster_by_category(df, categorical_column):
    plt.figure(figsize=(10, 6))

    unique_categories = sorted(df_clean[categorical_column].unique())
    palette = ['blue', 'navy']

    ax = sns.countplot(
        y='Cluster',
        hue=categorical_column,
        palette=palette,
        data=df_clean
    )

    num_categories = len(unique_categories)
    num_clusters = df_clean['Cluster'].nunique()

    patches = ax.patches
    for i, patch in enumerate(patches):
        hue_category_index = i // num_clusters
        if hue_category_index == 0:
            patch.set_alpha(0.5)
        else:
            patch.set_alpha(1.0)

    handles, labels = ax.get_legend_handles_labels()
    labels = [str(cat) for cat in unique_categories]
    ax.legend(handles, labels, title=categorical_column, loc='best')
    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x/1000)}K'))

    plt.title(f'Cluster by {categorical_column}')
    plt.xlabel('Count')
    plt.ylabel('Cluster')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

# Create Cluster Plots for Age and Flight Distance
plt.figure(figsize=(10, 7))

df_feature_eng = df_original.copy()
df_feature_eng['Cluster'] = df_clean['Cluster']

sns.scatterplot(
    data=df_feature_eng,
    x='Age',
    y='Flight Distance',
    hue='Cluster',
    palette='Set1',
    alpha=0.6,
    s=50  # Marker size
)

# Clearly mark cluster centers
X_for_clustering = df_feature_eng[['Age', 'Flight Distance']]

# Compute centers
kmeans = KMeans(n_clusters=df_feature_eng['Cluster'].nunique(), random_state=42)
kmeans.fit(X_for_clustering)
centers = kmeans.cluster_centers_

# Plot cluster centers clearly
plt.scatter(centers[:, 0], centers[:, 1], s=200, c='black', marker='X', label='Cluster Centers')

plt.title('Clusters of Passengers by Age and Flight Distance', fontsize=14)
plt.xlabel('Age', fontsize=12)
plt.ylabel('Flight Distance', fontsize=12)
plt.legend(title='Cluster')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

plot_cluster_by_category(df_clean, 'Gender')

def plot_cluster_by_category(df, categorical_column, legend_table):
    plt.figure(figsize=(10, 6))

    mapping_df = legend_table[legend_table["Column Name"] == categorical_column]
    value_mapping = dict(zip(mapping_df["Encoded Value"], mapping_df["Original Value"]))

    df_temp = df.copy()
    df_temp[categorical_column] = df_temp[categorical_column].map(value_mapping)

    unique_categories = list(df_temp[categorical_column].unique())
    palette = ['blue', 'navy'] if len(unique_categories) == 2 else sns.color_palette('husl', len(unique_categories))

    ax = sns.countplot(
        y='Cluster',
        hue=categorical_column,
        palette=palette,
        data=df_temp
    )

    num_clusters = df_temp['Cluster'].nunique()
    for i, patch in enumerate(ax.patches):
        hue_category_index = i // num_clusters
        patch.set_alpha(0.5 if hue_category_index == 0 else 1.0)

    handles, _ = ax.get_legend_handles_labels()
    ax.legend(handles, unique_categories, title=categorical_column, loc='best')
    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x/1000)}K'))

    plt.title(f'Cluster by {categorical_column}')
    plt.xlabel('Count')
    plt.ylabel('Cluster')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

plot_cluster_by_category(df_clean, 'Gender', legend_table)

plot_cluster_by_category(df_clean, 'Satisfaction', legend_table)

plot_cluster_by_category(df_clean, 'Type of Travel', legend_table)

def plot_cluster_by_category(df, categorical_column):
    plt.figure(figsize=(10, 6))

    unique_categories = sorted(df[categorical_column].unique())

    palette = sns.color_palette("Blues_d", n_colors=len(unique_categories))

    ax = sns.countplot(
        y='Cluster',
        hue=categorical_column,
        palette=palette,
        data=df
    )

    num_clusters = df['Cluster'].nunique()
    for i, patch in enumerate(ax.patches):
        hue_category_index = i // num_clusters
        patch.set_alpha(0.5 if hue_category_index == 0 else 1.0)

    handles, _ = ax.get_legend_handles_labels()
    ax.legend(handles, unique_categories, title=categorical_column, loc='best')
    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x/1000)}K'))

    plt.title(f'Cluster by {categorical_column}')
    plt.xlabel('Count')
    plt.ylabel('Cluster')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

df_cluster_analysis = df_clean.copy()

online_boarding_cols = [col for col in df_cluster_analysis.columns if col.startswith('Online Boarding_')]
df_cluster_analysis['Online Boarding (Original)'] = df_cluster_analysis[online_boarding_cols].idxmax(axis=1).str.split('_').str[-1].astype(int)

plot_cluster_by_category(df_cluster_analysis, 'Online Boarding (Original)')

def plot_cluster_by_category(df, categorical_column):
    original_col_name = categorical_column + ' (Original)'
    plotting_column = original_col_name if original_col_name in df.columns else categorical_column

    if plotting_column in df.columns:
        unique_categories = sorted(df[plotting_column].unique())
    else:
        print(
            f"Warning: Column '{categorical_column}' and its encoded versions not found in DataFrame. Skipping plot.")
        return

    palette = sns.color_palette("Blues_d", n_colors=len(unique_categories))

    plt.figure(figsize=(10, 6))
    ax = sns.countplot(
        y='Cluster',
        hue=plotting_column,
        palette=palette,
        data=df
    )

    num_clusters = df['Cluster'].nunique()
    for i, patch in enumerate(ax.patches):
        hue_category_index = i // num_clusters
        patch.set_alpha(0.5 if hue_category_index == 0 else 1.0)

    handles, _ = ax.get_legend_handles_labels()
    ax.legend(handles, unique_categories,
              title=categorical_column, loc='best')

    ax.xaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, _: f'{int(x/1000)}K'))

    plt.title(f'Cluster by {categorical_column}')
    plt.xlabel('Count')
    plt.ylabel('Cluster')
    plt.grid(True, linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.show()


def revert_one_hot_encoding(df):
    relevant_cols = ['Online Boarding', 'Type of Travel',
                    'In-flight Wifi Service', 'In-flight Service']
    for col in relevant_cols:
        encoded_cols = [c for c in df.columns if c.startswith(
            col + '_')]
        if encoded_cols:
            original_col_name = col + ' (Original)'
            if original_col_name not in df.columns:
                df[original_col_name] = pd.Series(
                    dtype='object')

                for encoded_col in encoded_cols:
                    original_value = encoded_col.split('_')[
                        1]

                    try:
                        original_value = int(
                            original_value)
                    except ValueError:
                        pass

                    condition = df[encoded_col] == 1
                    df.loc[condition,
                           original_col_name] = original_value

                if col == 'In-flight Service':
                    df[original_col_name] = pd.to_numeric(
                        df[original_col_name], errors='coerce').astype(pd.Int64Dtype())
    return df

df_cluster_analysis = revert_one_hot_encoding(df_cluster_analysis)
plot_cluster_by_category(
    df_cluster_analysis, 'In-flight Wifi Service')

# Examining the Clusters by Age
original_age_values = scaler.inverse_transform(df_cluster_analysis[numerical_cols])[:, numerical_cols.index('Age')]

df_cluster_analysis['Age (Original)'] = original_age_values

fig, axes = plt.subplots(1, df_cluster_analysis['Cluster'].nunique(), figsize=(15, 5), sharey=True)
axes = axes.flatten() if df_cluster_analysis['Cluster'].nunique() > 1 else [axes]

for i, cluster in enumerate(df_cluster_analysis['Cluster'].unique()):
    cluster_data = df_cluster_analysis[df_cluster_analysis['Cluster'] == cluster]
    sns.histplot(cluster_data['Age (Original)'], bins=20, kde=True, color='blue', alpha=0.5, ax=axes[i])
    axes[i].set_title(f"Cluster {cluster}")
    axes[i].set_xlabel("Age")
    axes[i].set_ylabel("Count" if i == 0 else "")
    axes[i].set_xlim(0, 100)
    axes[i].lines[0].set_color('navy')


fig.suptitle('Distribution of Age by Cluster', fontsize=16, y=1.05, ha='center')
plt.tight_layout()
plt.show()

"""## Feature Engineering"""

# Create a Feature Engineering visualisation for Long Delay (>30 mins) for
# Satisfaction
df_feature_eng = df_original.copy()

df_feature_eng["Long Delay"] = (df_feature_eng["Arrival Delay"] > 30).astype(int)

plt.figure(figsize=(8, 6))
sns.countplot(data=df_feature_eng,
              x="Long Delay",
              hue="Satisfaction",
              palette="coolwarm")
plt.xticks([0, 1], ['Arrival Delay ≤ 30 mins', 'Arrival Delay > 30 mins'])
plt.xlabel("Long Delay")
plt.ylabel("Number of Passengers")
plt.title("Impact of Long Arrival Delay (>30 mins) on Passenger Satisfaction")
plt.legend(title='Satisfaction')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Create a column for 'Loyal, Business Traveller'
df_feature_eng["Loyal Business Traveler"] = (
    (df_feature_eng["Customer Type"] == "Returning") &
    (df_feature_eng["Type of Travel"] == "Business")
).astype(int)

print(df_feature_eng["Loyal Business Traveler"].value_counts().astype(int))

plt.figure(figsize=(8, 6))
sns.countplot(data=df_feature_eng,
              x="Loyal Business Traveler",
              hue="Satisfaction",
              palette="pastel")
plt.xticks([0, 1], ['Others', 'Loyal Business Travelers'])
plt.xlabel("Traveler Type")
plt.ylabel("Number of Passengers")
plt.title("Satisfaction by Traveler Type (Loyal Business Travelers vs Others)")
plt.legend(title='Satisfaction')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

!pip install streamlit
!pip install pyngrok

import streamlit as st
import pandas as pd

# Set page configuration and title (across all pages)
st.set_page_config(page_title="Airline Customer Satisfaction", layout="wide")

# Initialise session state for page navigation
if 'page' not in st.session_state:
    st.session_state.page = 'EDA'

# Define header
st.header("Airline Customer Satisfaction")

# Navigation buttons
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("Exploratory Data Analysis (EDA)"):
        st.session_state.page = 'EDA'
with col2:
    if st.button("Supervised Machine Learning"):
        st.session_state.page = 'Supervised ML'
with col3:
    if st.button("Unsupervised Machine Learning - KMeans"):
        st.session_state.page = 'Unsupervised ML'

st.markdown("---")

# Exploratory Data Analysis (EDA) Page
if st.session_state.page == 'EDA':
    st.subheader("Exploratory Data Analysis (EDA)")
    if st.checkbox("Show Raw Dataset (First 100 rows)"):
        st.dataframe(df.head(100))

    '''selected_column = st.selectbox("Select Column for Distribution", df.select_dtypes(include=['int64', 'float64']).columns)
    st.bar_chart(df[selected_column].value_counts())'''

# Supervised Machine Learning Page
elif st.session_state.page == 'Supervised ML':
    st.subheader("Supervised Machine Learning")
    st.write("Supervised Machine Learning analysis results here.")

    # Placeholder for your supervised learning results (e.g., metrics)
    metrics = {
        "Accuracy": 0.95,
        "Precision": 0.94,
        "Recall": 0.93,
        "F1-Score": 0.94,
        "ROC AUC": 0.96
    }
    st.write(metrics)

    st.write("*(Replace this section with your actual ML visualizations and outputs.)*")

# Unsupervised Machine Learning (KMeans) Page
elif st.session_state.page == 'Unsupervised ML':
    st.subheader("Unsupervised Machine Learning - KMeans")
    st.write("Unsupervised KMeans Clustering results here.")

    # Placeholder for your clustering results (e.g., cluster distribution)
    st.write(df['Class'].value_counts())

    st.write("*(Replace this section with your actual KMeans visualizations and outputs.)*")

